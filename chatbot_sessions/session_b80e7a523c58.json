{
  "session_id": "session_b80e7a523c58",
  "current_stage": "awaiting_content",
  "messages": [
    {
      "message_id": "msg_1761394518.779415",
      "message_type": "user",
      "content": "Please write a linkedin piost for thios python file",
      "timestamp": "2025-10-25T17:45:18.779431",
      "metadata": {}
    },
    {
      "message_id": "msg_1761394526.203404",
      "message_type": "assistant",
      "content": "âŒ Blog generation failed: Generation failed: Failed to parse BlogPost from completion {\"hook\": \"What if you could revolutionize your research workflow with AI?\", \"content\": \"I've been experimenting with a Deep Research AI Agent System that leverages LangGraph, LangChain, and multiple AI models like Claude, GPT-4o, and Perplexity. This system enables complex workflow management, conditional workflows, and AI model diversity, leading to more comprehensive research outcomes. Key benefits include: \\n   - Improved research planning and execution\\n   - Enhanced content analysis and fact-checking\\n   - Increased efficiency through automated workflows\", \"hashtags\": [\"#AIforResearch\", \"#LangGraph\", \"#LangChain\", \"#Perplexity\", \"#Claude\", \"#GPT4o\", \"#ResearchInnovation\", \"#WorkflowAutomation\"], \"cta\": \"How do you think AI can transform your research workflow? Share your thoughts!\", \"engagementScore\": 8}. Got: 2 validation errors for BlogPost\ntitle\n  Field required [type=missing, input_value={'hook': 'What if you cou...', 'engagementScore': 8}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\ncall_to_action\n  Field required [type=missing, input_value={'hook': 'What if you cou...', 'engagementScore': 8}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
      "timestamp": "2025-10-25T17:45:26.203415",
      "metadata": {}
    }
  ],
  "blog_context": {
    "source_type": "file",
    "source_path": "/var/folders/zp/ck9fmcw91x72gsm7z8t9nq7h0000gn/T/test123.py",
    "source_content": "# Deep Research AI Agent System Implementation\n# Using LangGraph and LangChain with Tavily and Perplexity integration\n\nimport os\nimport json\nimport asyncio\nimport concurrent.futures\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\n\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_perplexity import ChatPerplexity\nimport langgraph as lg\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom dotenv import load_dotenv\nimport requests\nfrom bs4 import BeautifulSoup\nfrom readability import Document\nimport dateutil.parser\n\n# Load environment variables\nload_dotenv()\n\n# Configuration and Models\nTAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\nPERPLEXITY_API_KEY = os.getenv(\"PERPLEXITY_API_KEY\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n\n# Use Claude for primary reasoning and content analysis\nCLAUDE_MODEL = ChatAnthropic(model=\"claude-3-7-sonnet-20250219\", temperature=0.2)\n# Fallback to GPT-4o for certain tasks\nGPT_MODEL = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n# Initialize Perplexity Chat\nPERPLEXITY_MODEL = ChatPerplexity(api_key=PERPLEXITY_API_KEY, model=\"pplx-70b-online\")\n\n# Select primary model\nMODEL = CLAUDE_MODEL\n\n# State Management - single shared state across all agents\nclass ResearchState(BaseModel):\n    query: str = Field(description=\"The original research query\")\n    research_plan: str = Field(default=\"\", description=\"The plan for conducting the research\")\n    search_queries: List[str] = Field(default_factory=list, description=\"List of search queries to execute\")\n    requires_metrics: bool = Field(default=False, description=\"Whether the query explicitly requires quantitative metrics\")\n    perplexity_results: Dict[str, Any] = Field(default_factory=dict, description=\"Results from Perplexity API\")\n    search_results: List[Dict] = Field(default_factory=list, description=\"Raw search results from Tavily\")\n    content_details: List[Dict] = Field(default_factory=list, description=\"Full content extracted from URLs\")\n    analyzed_content: Dict[str, Any] = Field(default_factory=dict, description=\"Synthesized information and findings\")\n    draft_answer: str = Field(default=\"\", description=\"Draft answer to the original query\")\n    verified_info: Dict[str, Any] = Field(default_factory=dict, description=\"Fact-checked information\")\n    final_answer: str = Field(default=\"\", description=\"Final polished answer\")\n    citations: List[Dict[str, str]] = Field(default_factory=list, description=\"Tracked citations for sources\")\n    metadata: Dict = Field(default_factory=dict, description=\"Process metadata and timestamps\")\n\n# Tools\n@tool\ndef tavily_search(query: str, time_range: str = \"auto\", search_depth: str = \"advanced\") -> List[Dict]:\n    \"\"\"Search the web using Tavily API with configurable parameters.\n    \n    Args:\n        query: The search query\n        time_range: Time range for results (auto, day, week, month, year)\n        search_depth: Depth of search (basic or advanced)\n    \n    Returns:\n        List of search results\n    \"\"\"\n    search = TavilySearchResults(\n        api_key=TAVILY_API_KEY,\n        k=7,\n        include_domains=[],\n        exclude_domains=[],\n        search_depth=search_depth,\n        include_raw_content=True,\n        include_images=False,\n        max_results=7,\n    )\n    results = search.invoke({\"query\": query, \"search_depth\": search_depth})\n    return results\n\n@tool\ndef tavily_extract_content(url: str) -> Dict[str, Any]:\n    \"\"\"Extract content from a URL using Tavily's content extraction API.\n    \n    Args:\n        url: The URL to extract content from\n        \n    Returns:\n        Dictionary with extracted content and metadata\n    \"\"\"\n    headers = {\"Content-Type\": \"application/json\", \"x-api-key\": TAVILY_API_KEY}\n    payload = {\"url\": url, \"include_images\": False}\n    \n    try:\n        response = requests.post(\n            \"https://api.tavily.com/extract\",\n            headers=headers,\n            json=payload,\n            timeout=15\n        )\n        \n        if response.status_code == 200:\n            data = response.json()\n            return {\n                \"url\": url,\n                \"title\": data.get(\"title\", \"\"),\n                \"text\": data.get(\"content\", \"\"),\n                \"publish_date\": data.get(\"publish_date\", \"\"),\n                \"status\": \"success\"\n            }\n        else:\n            return {\n                \"url\": url, \n                \"status\": \"failed\", \n                \"error\": f\"Status code: {response.status_code}\"\n            }\n    except Exception as e:\n        return {\"url\": url, \"status\": \"failed\", \"error\": str(e)}\n\n@tool\ndef perplexity_search(query: str, focus: str = \"normal\") -> Dict[str, Any]:\n    \"\"\"Search the web using Perplexity API for comprehensive summaries.\n    \n    Args:\n        query: The search query\n        focus: Focus mode (normal, concise, comprehensive)\n        \n    Returns:\n        Dictionary with Perplexity response including text and sources\n    \"\"\"\n    if not PERPLEXITY_API_KEY:\n        return {\n            \"text\": \"Perplexity API key not configured. Skipping this step.\",\n            \"sources\": [],\n            \"status\": \"failed\"\n        }\n    \n    try:\n        # Use the LangChain ChatPerplexity integration\n        response = PERPLEXITY_MODEL.invoke(query)\n        \n        # Extract source information if available\n        sources = []\n        if hasattr(response, 'additional_kwargs') and 'sources' in response.additional_kwargs:\n            sources = response.additional_kwargs['sources']\n        \n        return {\n            \"text\": response.content,\n            \"sources\": sources,\n            \"status\": \"success\"\n        }\n    except Exception as e:\n        return {\n            \"text\": f\"Error: {str(e)}\",\n            \"sources\": [],\n            \"status\": \"failed\"\n        }\n\n@tool\ndef extract_content_from_url(url: str) -> str:\n    \"\"\"Extract content from a URL using BeautifulSoup with readability enhancement.\n    Used as fallback when Tavily extraction isn't available.\n    \"\"\"\n    try:\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n        }\n        response = requests.get(url, headers=headers, timeout=10)\n        if response.status_code == 200:\n            # First use readability to get the main content\n            doc = Document(response.text)\n            title = doc.title()\n            readable_content = doc.summary()\n            \n            # Then extract text from the readability HTML\n            soup = BeautifulSoup(readable_content, 'html.parser')\n            \n            # Get text content\n            text = soup.get_text(separator='\\n', strip=True)\n            \n            # Attempt to extract publish date\n            try:\n                publish_date = \"\"\n                date_meta = soup.select_one('meta[property=\"article:published_time\"]')\n                if date_meta:\n                    publish_date = date_meta.get('content', '')\n                else:\n                    # Try other common date meta tags\n                    date_candidates = [\n                        soup.select_one('meta[name=\"date\"]'),\n                        soup.select_one('meta[name=\"pubdate\"]'),\n                        soup.select_one('time'),\n                    ]\n                    for candidate in date_candidates:\n                        if candidate:\n                            date_str = candidate.get('content') or candidate.get('datetime') or ''\n                            if date_str:\n                                publish_date = date_str\n                                break\n            except:\n                publish_date = \"\"\n            \n            # Truncate if too long but keep more content for Claude's larger context window\n            max_length = 15000\n            if len(text) > max_length:\n                text = text[:max_length] + \"... [content truncated]\"\n                \n            return {\n                \"title\": title,\n                \"text\": text,\n                \"publish_date\": publish_date,\n                \"status\": \"success\"\n            }\n        return {\n            \"title\": \"\",\n            \"text\": f\"Failed to retrieve content: Status code {response.status_code}\",\n            \"publish_date\": \"\",\n            \"status\": \"failed\"\n        }\n    except Exception as e:\n        return {\n            \"title\": \"\",\n            \"text\": f\"Error extracting content: {str(e)}\",\n            \"publish_date\": \"\",\n            \"status\": \"failed\"\n        }\n\n# Agent Nodes\n\ndef planner_agent(state: ResearchState) -> ResearchState:\n    \"\"\"Plan the research strategy and generate search queries with special focus on metrics if needed.\"\"\"\n    # First, analyze if the query requires quantitative metrics\n    metrics_check_prompt = ChatPromptTemplate.from_template(\"\"\"\n    Analyze the following query and determine if it explicitly asks for quantitative metrics,\n    numbers, statistics, or comparative data.\n    \n    Query: {query}\n    \n    Return your analysis as a JSON:\n    {{\n        \"requires_metrics\": true/false,\n        \"metric_subjects\": [\"subject1\", \"subject2\"],  // What entities need metrics\n        \"metric_types\": [\"type1\", \"type2\"]  // What kinds of metrics are needed (parameters, benchmarks, etc.)\n    }}\n    \"\"\")\n    \n    response = MODEL.invoke([HumanMessage(content=metrics_check_prompt.format(query=state.query))])\n    \n    try:\n        # Extract JSON content\n        json_content = response.content\n        if \"```json\" in json_content:\n            json_content = json_content.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in json_content:\n            json_content = json_content.split(\"```\")[1].split(\"```\")[0].strip()\n            \n        metrics_analysis = json.loads(json_content)\n        state.requires_metrics = metrics_analysis.get(\"requires_metrics\", False)\n        state.metadata[\"metric_subjects\"] = metrics_analysis.get(\"metric_subjects\", [])\n        state.metadata[\"metric_types\"] = metrics_analysis.get(\"metric_types\", [])\n    except:\n        # Fallback\n        state.requires_metrics = False\n    \n    # Now generate the research plan and queries\n    planning_prompt = ChatPromptTemplate.from_template(\"\"\"\n    You are a research planning expert. Given the following query, develop a comprehensive\n    research plan and generate specific search queries to gather information.\n    \n    Query: {query}\n    \n    {metrics_instruction}\n    \n    First, create a detailed research plan that includes:\n    1. Main aspects of the topic that need to be researched\n    2. Key information that must be gathered\n    3. Specific angles to investigate\n    \n    Then, generate these specific search queries:\n    1. One broad query to get an overview of the topic\n    2. 3-5 targeted queries for specific aspects mentioned in the query\n    {metrics_queries}\n    \n    Return your response as a JSON with the following format:\n    {{\n        \"research_plan\": \"detailed research plan here\",\n        \"search_queries\": [\"query1\", \"query2\", \"query3\", ...]\n    }}\n    \"\"\")\n    \n    # Add special instructions if metrics are required\n    metrics_instruction = \"\"\n    metrics_queries = \"\"\n    if state.requires_metrics:\n        metrics_instruction = \"\"\"\n        This query explicitly asks for quantitative metrics, numbers, and/or comparative data.\n        Your research plan MUST focus on finding specific numerical data, benchmarks, and statistics.\n        \"\"\"\n        \n        metrics_queries = \"\"\"\n        3. 2-3 queries specifically aimed at finding numerical data, using terms like:\n           - \"[subject] performance metrics\"\n           - \"[subject] benchmarks\"\n           - \"[subject] parameters comparison\"\n           - \"[subject] technical specifications\"\n        4. Add site-specific searches for data sources like:\n           - \"site:paperswithcode.com [subject] benchmarks\"\n           - \"site:huggingface.co/docs [subject] parameters\"\n        \"\"\"\n    \n    response = MODEL.invoke([\n        HumanMessage(content=planning_prompt.format(\n            query=state.query,\n            metrics_instruction=metrics_instruction,\n            metrics_queries=metrics_queries\n        ))\n    ])\n    \n    try:\n        # Extract JSON content\n        json_content = response.content\n        if \"```json\" in json_content:\n            json_content = json_content.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in json_content:\n            json_content = json_content.split(\"```\")[1].split(\"```\")[0].strip()\n            \n        result = json.loads(json_content)\n        state.research_plan = result[\"research_plan\"]\n        state.search_queries = result[\"search_queries\"]\n    except:\n        # Fallback handling\n        state.research_plan = \"General research on the topic\"\n        state.search_queries = [state.query]\n    \n    state.metadata[\"planning_timestamp\"] = datetime.now().isoformat()\n    state.metadata[\"requires_metrics\"] = state.requires_metrics\n    \n    return state\n\ndef search_agent(state: ResearchState) -> ResearchState:\n    \"\"\"Execute search queries using Tavily and Perplexity with intelligent routing.\"\"\"\n    all_results = []\n    \n    # First, get a broad overview from Perplexity for the main query\n    if PERPLEXITY_API_KEY:\n        state.metadata[\"search_strategy\"] = \"hybrid_perplexity_tavily\"\n        \n        # Use Perplexity for the main query to get an overview\n        main_query = state.query\n        perplexity_result = perplexity_search(main_query)\n        \n        state.perplexity_results = perplexity_result\n        \n        # Extract cited URLs from Perplexity to avoid duplicating searches\n        perplexity_urls = set()\n        for source in perplexity_result.get(\"sources\", []):\n            if isinstance(source, dict) and \"url\" in source:\n                perplexity_urls.add(source[\"url\"])\n            elif isinstance(source, str) and (source.startswith(\"http://\") or source.startswith(\"https://\")):\n                perplexity_urls.add(source)\n                \n        # Add source information to citations tracking\n        for i, source in enumerate(perplexity_result.get(\"sources\", [])):\n            if isinstance(source, dict):\n                title = source.get(\"title\", \"Untitled\")\n                url = source.get(\"url\", \"\")\n            elif isinstance(source, str) and (source.startswith(\"http://\") or source.startswith(\"https://\")):\n                title = f\"Source {i+1}\"\n                url = source\n            else:\n                continue\n            \n            state.citations.append({\n                \"id\": f\"p{i+1}\",\n                \"title\": title,\n                \"url\": url,\n                \"source_type\": \"perplexity\"\n            })\n    else:\n        state.metadata[\"search_strategy\"] = \"tavily_only\"\n    \n    # Use Tavily for specific queries\n    for i, query in enumerate(state.search_queries):\n        # Execute the search\n        results = tavily_search({\"query\": query, \"time_range\": \"auto\", \"search_depth\": \"advanced\"})\n        \n        # Add to results, marking which query produced them\n        for result in results:\n            result[\"query_source\"] = query\n            result[\"query_index\"] = i\n            all_results.append(result)\n    \n    # Remove duplicates by URL\n    unique_results = []\n    seen_urls = set()\n    \n    for result in all_results:\n        url = result.get(\"url\")\n        if url and url not in seen_urls:\n            # Skip URLs already found in Perplexity results to avoid duplication\n            if url in perplexity_urls:\n                continue\n                \n            unique_results.append(result)\n            seen_urls.add(url)\n    \n    state.search_results = unique_results\n    state.metadata[\"search_timestamp\"] = datetime.now().isoformat()\n    state.metadata[\"tavily_results_count\"] = len(unique_results)\n    state.metadata[\"perplexity_sources_count\"] = len(state.perplexity_results.get(\"sources\", []))\n    \n    return state\n\ndef content_extraction_agent(state: ResearchState) -> ResearchState:\n    \"\"\"Extract full content from search result URLs with parallel processing.\"\"\"\n    content_details = []\n    \n    # Add Perplexity summary as a special source\n    if state.perplexity_results and state.perplexity_results.get(\"status\") == \"success\":\n        content_details.append({\n            \"url\": \"perplexity_summary\",\n            \"title\": \"Perplexity Summary\",\n            \"snippet\": state.perplexity_results.get(\"text\", \"\")[:500] + \"...\",\n            \"full_content\": state.perplexity_results.get(\"text\", \"\"),\n            \"publish_date\": datetime.now().isoformat(),\n            \"source_type\": \"perplexity_summary\"\n        })\n    \n    # Process sources from Perplexity\n    perplexity_sources = state.perplexity_results.get(\"sources\", [])\n    perplexity_urls = []\n    \n    for source in perplexity_sources:\n        if \"url\" in source and \"title\" in source:\n            perplexity_urls.append({\n                \"url\": source[\"url\"],\n                \"title\": source.get(\"title\", \"Untitled\"),\n                \"snippet\": source.get(\"snippet\", \"\"),\n                \"source_type\": \"perplexity\"\n            })\n    \n    # Combine Perplexity and Tavily sources, prioritizing sources that might have metrics\n    # for metric queries\n    all_sources = []\n    \n    # First add Perplexity sources\n    all_sources.extend(perplexity_urls)\n    \n    # Then add top Tavily results\n    for result in state.search_results[:7]:  # Limit to top 7 results\n        all_sources.append({\n            \"url\": result.get(\"url\"),\n            \"title\": result.get(\"title\", \"Untitled\"),\n            \"snippet\": result.get(\"content\", \"\"),\n            \"source_type\": \"tavily\"\n        })\n    \n    # For metrics queries, prioritize sources that appear to contain data\n    if state.requires_metrics:\n        # Prioritize sources that mention metrics, benchmarks, etc.\n        data_terms = [\"benchmark\", \"metric\", \"performance\", \"comparison\", \"parameter\", \n                     \"statistic\", \"score\", \"accuracy\", \"evaluation\", \"leaderboard\"]\n        \n        for source in all_sources:\n            source[\"priority\"] = 0\n            snippet = source.get(\"snippet\", \"\").lower()\n            title = source.get(\"title\", \"\").lower()\n            \n            # Check for data indicators in title and snippet\n            for term in data_terms:\n                if term in title:\n                    source[\"priority\"] += 3\n                if term in snippet:\n                    source[\"priority\"] += 1\n                    \n            # Extra boost for HuggingFace, PapersWithCode and other data-rich sources\n            url = source.get(\"url\", \"\").lower()\n            if any(domain in url for domain in [\"huggingface\", \"paperswithcode\", \"leaderboard\", \n                                              \"benchmark\", \"kaggle\", \"github\"]):\n                source[\"priority\"] += 5\n        \n        # Sort by priority for metrics queries\n        all_sources.sort(key=lambda x: x.get(\"priority\", 0), reverse=True)\n    \n    # Function to extract content from a single URL\n    def extract_single_url(source_info):\n        url = source_info[\"url\"]\n        title = source_info[\"title\"]\n        snippet = source_info[\"snippet\"]\n        source_type = source_info[\"source_type\"]\n        \n        # Skip the perplexity summary as it's already processed\n        if url == \"perplexity_summary\":\n            return None\n            \n        try:\n            # Try Tavily extraction first\n            extraction_result = tavily_extract_content(url)\n            \n            # If Tavily extraction failed, fall back to regular extraction\n            if extraction_result.get(\"status\") == \"failed\":\n                extraction_result = extract_content_from_url(url)\n                \n            # Ensure we have the title, even if extraction failed\n            if not extraction_result.get(\"title\"):\n                extraction_result[\"title\"] = title\n                \n            # Add source metadata\n            extraction_result[\"url\"] = url\n            extraction_result[\"snippet\"] = snippet\n            extraction_result[\"source_type\"] = source_type\n            \n            return extraction_result\n        except Exception as e:\n            # Return error information\n            return {\n                \"url\": url,\n                \"title\": title,\n                \"snippet\": snippet,\n                \"full_content\": f\"Error extracting content: {str(e)}\",\n                \"status\": \"failed\",\n                \"source_type\": source_type\n            }\n    \n    # Process URLs in parallel to improve performance\n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        # Limit to top 12 sources (combined from Perplexity and Tavily)\n        sources_to_process = all_sources[:12]\n        future_to_url = {executor.submit(extract_single_url, source): source for source in sources_to_process}\n        \n        for future in concurrent.futures.as_completed(future_to_url):\n            source = future_to_url[future]\n            try:\n                result = future.result()\n                if result:  # Skip None results (like the perplexity summary)\n                    # Add full content extraction results\n                    content_details.append({\n                        \"url\": result.get(\"url\"),\n                        \"title\": result.get(\"title\", source.get(\"title\", \"Untitled\")),\n                        \"snippet\": source.get(\"snippet\", \"\"),\n                        \"full_content\": result.get(\"text\", \"\"),\n                        \"publish_date\": result.get(\"publish_date\", \"\"),\n                        \"source_type\": source.get(\"source_type\"),\n                        \"status\": result.get(\"status\", \"unknown\")\n                    })\n                    \n                    # Add to citations if not already there\n                    url = result.get(\"url\")\n                    if url and not any(citation[\"url\"] == url for citation in state.citations):\n                        citation_id = f\"t{len(state.citations) + 1}\"\n                        state.citations.append({\n                            \"id\": citation_id,\n                            \"title\": result.get(\"title\", \"Untitled\"),\n                            \"url\": url,\n                            \"source_type\": source.get(\"source_type\")\n                        })\n            except Exception as e:\n                print(f\"Error processing {source.get('url')}: {e}\")\n    \n    state.content_details = content_details\n    state.metadata[\"extraction_timestamp\"] = datetime.now().isoformat()\n    state.metadata[\"extracted_sources_count\"] = len(content_details)\n    \n    return state\n\ndef analysis_agent(state: ResearchState) -> ResearchState:\n    \"\"\"Synthesize and evaluate the extracted information with focus on metrics for queries requiring them.\"\"\"\n    if not state.content_details:\n        state.analyzed_content = {\n            \"key_findings\": [\"No content was found.\"],\n            \"information_gaps\": [\"Unable to find information on the query.\"]\n        }\n        return state\n    \n    # Prepare context for analysis, prioritizing successful extractions\n    successful_sources = [s for s in state.content_details if s.get(\"status\") != \"failed\"]\n    failed_sources = [s for s in state.content_details if s.get(\"status\") == \"failed\"]\n    \n    # Use all successful sources and add a note about failed ones\n    sources_to_analyze = successful_sources\n    if failed_sources:\n        failed_note = f\"Note: {len(failed_sources)} sources could not be accessed.\"\n    else:\n        failed_note = \"\"\n    \n    # Prepare the context with source content\n    # For metrics queries, we want to preserve more numerical data in the context\n    max_content_per_source = 2500 if state.requires_metrics else 1000\n    \n    content_context = \"\\n\\n\".join([\n        f\"Source {i+1} [ID:{state.citations[i]['id'] if i < len(state.citations) else 'unknown'}]:\\n\"\n        f\"Title: {item['title']}\\n\"\n        f\"URL: {item['url']}\\n\"\n        f\"Date: {item.get('publish_date', 'Unknown')}\\n\"\n        f\"Content Summary: {item['snippet']}\\n\"\n        f\"Full Content Excerpt: {item['full_content'][:max_content_per_source]}...\"\n        for i, item in enumerate(sources_to_analyze)\n    ])\n    \n    # Add failed sources note\n    if failed_note:\n        content_context += f\"\\n\\n{failed_note}\"\n    \n    # Create the appropriate prompt based on query type\n    if state.requires_metrics:\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an expert research analyst specializing in quantitative data analysis. \n        The user query explicitly asks for metrics, numbers, and quantitative comparisons.\n        Analyze the following content focusing on extracting specific metrics, numbers, benchmarks,\n        parameters, and quantitative data to answer the query.\n        \n        Original Query: {query}\n        Research Plan: {research_plan}\n        \n        Content to Analyze:\n        {content}\n        \n        Provide your analysis in JSON format with the following structure:\n        {{\n            \"key_findings\": [\n                \"finding1 (with specific metrics where available)\",\n                \"finding2 (with specific metrics where available)\",\n                ...\n            ],\n            \"quantitative_data\": [\n                {{\n                    \"entity\": \"Name of entity (e.g., model name)\",\n                    \"metric_name\": \"Name of metric (e.g., MMLU score)\",\n                    \"value\": \"Numerical or formatted value\", \n                    \"unit\": \"Unit if applicable\",\n                    \"source_id\": \"ID of source that provided this data\"\n                }},\n                ...\n            ],\n            \"comparative_analysis\": \"Analysis comparing the metrics across entities\",\n            \"main_themes\": [\"theme1\", \"theme2\", ...],\n            \"information_gaps\": [\"gap1\", \"gap2\", ...],\n            \"source_assessment\": \"Assessment of source quality and data reliability\"\n        }}\n        \n        IMPORTANT:\n        1. Extract ALL specific numbers, metrics, and quantitative data found in the sources\n        2. When listing metrics, always cite which source provided each data point\n        3. If sources give conflicting metrics, note the discrepancy and provide both with sources\n        4. Include specific values wherever possible (e.g., \"GPT-4 has 1.7 trillion parameters\" is better than \"GPT-4 has many parameters\")\n        \"\"\")\n    else:\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an expert research analyst. Analyze the following content and extract \n        key insights related to the original query.\n        \n        Original Query: {query}\n        Research Plan: {research_plan}\n        \n        Content to Analyze:\n        {content}\n        \n        Provide your analysis in JSON format with the following structure:\n        {{\n            \"key_findings\": [\"finding1\", \"finding2\", ...],\n            \"main_themes\": [\"theme1\", \"theme2\", ...],\n            \"information_gaps\": [\"gap1\", \"gap2\", ...],\n            \"source_assessment\": \"brief assessment of source quality\"\n        }}\n        \n        When extracting findings:\n        1. Include specific data points and numbers when available\n        2. Note any conflicting information between sources\n        3. Prioritize recent information when dates are available\n        \"\"\")\n    \n    response = MODEL.invoke([\n        HumanMessage(content=prompt.format(\n            query=state.query,\n            research_plan=state.research_plan,\n            content=content_context\n        ))\n    ])\n    \n    try:\n        # Extract JSON content\n        json_content = response.content\n        if \"```json\" in json_content:\n            json_content = json_content.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in json_content:\n            json_content = json_content.split(\"```\")[1].split(\"```\")[0].strip()\n            \n        state.analyzed_content = json.loads(json_content)\n    except:\n        # Fallback if parsing fails\n        state.analyzed_content = {\n            \"key_findings\": [\"Analysis failed to parse results properly.\"],\n            \"main_themes\": [],\n            \"information_gaps\": [\"Technical error in analysis phase.\"],\n            \"source_assessment\": \"Unable to assess sources due to processing error.\"\n        }\n    \n    # Check for missing data that would be important for the query\n    missing_aspects_prompt = ChatPromptTemplate.from_template(\"\"\"\n    Based on the original query and the key findings so far, identify any important aspects \n    of the query that have not been addressed or require additional information.\n    \n    Original Query: {query}\n    \n    Key Findings So Far:\n    {findings}\n    \n    Information Gaps Already Identified:\n    {gaps}\n    \n    Identify any missing aspects that are important to fully answer the query.\n    Return as a JSON array of missing aspects:\n    [\"missing aspect 1\", \"missing aspect 2\", ...]\n    \n    If no important aspects are missing, return an empty array.\n    \"\"\")\n    \n    findings = \"\\n\".join([f\"- {finding}\" for finding in state.analyzed_content.get(\"key_findings\", [])])\n    gaps = \"\\n\".join([f\"- {gap}\" for gap in state.analyzed_content.get(\"information_gaps\", [])])\n    \n    response = MODEL.invoke([\n        HumanMessage(content=missing_aspects_prompt.format(\n            query=state.query,\n            findings=findings,\n            gaps=gaps\n        ))\n    ])\n    \n    try:\n        # Extract missing aspects\n        missing_content = response.content\n        if \"```json\" in missing_content:\n            missing_content = missing_content.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in missing_content:\n            missing_content = missing_content.split(\"```\")[1].split(\"```\")[0].strip()\n        \n        missing_aspects = json.loads(missing_content)\n        state.analyzed_content[\"missing_aspects\"] = missing_aspects\n    except:\n        state.analyzed_content[\"missing_aspects\"] = []\n    \n    state.metadata[\"analysis_timestamp\"] = datetime.now().isoformat()\n    return state\n\ndef drafting_agent(state: ResearchState) -> ResearchState:\n    \"\"\"Compose a draft answer based on the analyzed information with proper citations and metrics.\"\"\"\n    key_findings = \"\\n\".join([f\"- {finding}\" for finding in state.analyzed_content.get(\"key_findings\", [])])\n    information_gaps = \"\\n\".join([f\"- {gap}\" for gap in state.analyzed_content.get(\"information_gaps\", [])])\n    \n    # Format citations for reference\n    citations_info = \"\\n\".join([\n        f\"[{cite['id']}] {cite['title']} ({cite['url']})\"\n        for cite in state.citations\n    ])\n    \n    # Create the appropriate prompt based on query type\n    if state.requires_metrics:\n        # Get quantitative data if available\n        quant_data = state.analyzed_content.get(\"quantitative_data\", [])\n        formatted_quant_data = json.dumps(quant_data, indent=2) if quant_data else \"No specific quantitative data extracted.\"\n        \n        comparative_analysis = state.analyzed_content.get(\"comparative_analysis\", \"\")\n        \n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an expert content creator specializing in data-driven reports. Draft a comprehensive \n        answer to the original query with strong emphasis on quantitative data and metrics.\n        \n        Original Query: {query}\n        \n        Key Findings:\n        {key_findings}\n        \n        Quantitative Data:\n        {quant_data}\n        \n        Comparative Analysis:\n        {comparative_analysis}\n        \n        Information Gaps:\n        {information_gaps}\n        \n        Available Citations:\n        {citations}\n        \n        Draft a well-structured, data-rich answer that:\n        1. Addresses the query directly, emphasizing numerical data and metrics\n        2. Organizes information logically, possibly using tables or structured formats for metrics\n        3. Includes proper citations using the citation IDs provided (format: [ID])\n        4. Clearly presents comparative analysis between entities\n        5. Notes significant limitations or information gaps\n        6. Uses a clear, authoritative tone appropriate for technical/data content\n        \n        The final answer MUST include specific numbers, metrics, and quantitative data with citations.\n        When including a metric, ALWAYS add the citation ID in format [ID].\n        \"\"\")\n        \n        response = MODEL.invoke([\n            HumanMessage(content=prompt.format(\n                query=state.query,\n                key_findings=key_findings,\n                quant_data=formatted_quant_data,\n                comparative_analysis=comparative_analysis,\n                information_gaps=information_gaps,\n                citations=citations_info\n            ))\n        ])\n    else:\n        # Standard prompt for non-metric queries\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an expert content creator. Draft a comprehensive answer to the original query \n        based on the research findings.\n        \n        Original Query: {query}\n        \n        Key Findings:\n        {key_findings}\n        \n        Information Gaps:\n        {information_gaps}\n        \n        Available Citations:\n        {citations}\n        \n        Draft a well-structured answer that:\n        1. Addresses the query directly\n        2. Organizes information in a logical flow\n        3. Includes proper citations using the citation IDs provided (format: [ID])\n        4. Notes any significant limitations in the available information\n        5. Uses a clear, informative style\n        \n        When including important facts or claims, add the citation ID in format [ID].\n        \"\"\")\n        \n        response = MODEL.invoke([\n            HumanMessage(content=prompt.format(\n                query=state.query,\n                key_findings=key_findings,\n                information_gaps=information_gaps,\n                citations=citations_info\n            ))\n        ])\n    \n    state.draft_answer = response.content\n    state.metadata[\"drafting_timestamp\"] = datetime.now().isoformat()\n    return state\n\ndef fact_checking_agent(state: ResearchState) -> ResearchState:\n    \"\"\"Verify claims and sources in the draft answer with special attention to quantitative claims.\"\"\"\n    # Prepare context with key findings and quantitative data\n    key_findings = \"\\n\".join([f\"- {finding}\" for finding in state.analyzed_content.get(\"key_findings\", [])])\n    \n    # Add quantitative data context if available\n    quant_data_context = \"\"\n    if state.requires_metrics and \"quantitative_data\" in state.analyzed_content:\n        quant_data = state.analyzed_content[\"quantitative_data\"]\n        formatted_data = [\n            f\"- {item.get('entity', 'Unknown entity')}: {item.get('metric_name', 'Metric')}: \"\n            f\"{item.get('value', 'Unknown value')} {item.get('unit', '')}\"\n            f\" (Source: {item.get('source_id', 'unknown')})\"\n            for item in quant_data\n        ]\n        quant_data_context = \"\\n\".join(formatted_data)\n    \n    # Prepare citations context\n    citations_context = \"\\n\".join([\n        f\"[{cite['id']}] {cite['title']} ({cite['url']})\"\n        for cite in state.citations\n    ])\n    \n    # Create appropriate prompt based on query type\n    if state.requires_metrics:\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a fact-checking expert specializing in data verification. Review the draft answer \n        against the research findings with special attention to quantitative claims.\n        \n        Original Query: {query}\n        \n        Draft Answer:\n        {draft_answer}\n        \n        Key Findings from Research:\n        {key_findings}\n        \n        Quantitative Data from Research:\n        {quant_data}\n        \n        Citations Used:\n        {citations}\n        \n        Verify the following aspects of the draft:\n        1. ALL numerical claims are accurate and match the source data\n        2. ALL metrics, statistics, and quantities are correctly cited\n        3. Comparisons between entities are mathematically correct\n        4. No important quantitative findings were omitted\n        5. Citations are properly used\n        \n        Return your assessment in JSON format:\n        {{\n            \"accuracy_assessment\": \"overall assessment of factual accuracy\",\n            \"number_verification\": [\n                {{\n                    \"claim\": \"exact numerical claim from draft\",\n                    \"verification\": \"verified/incorrect/unsupported\",\n                    \"correction\": \"correction if needed\",\n                    \"note\": \"verification note\"\n                }},\n                ...\n            ],\n            \"missing_metrics\": [\"important metric 1 that was omitted\", ...],\n            \"citation_errors\": [\"description of citation error 1\", ...],\n            \"suggested_corrections\": [\"correction 1\", ...]\n        }}\n        \"\"\")\n        \n        response = MODEL.invoke([\n            HumanMessage(content=prompt.format(\n                query=state.query,\n                draft_answer=state.draft_answer,\n                key_findings=key_findings,\n                quant_data=quant_data_context,\n                citations=citations_context\n            ))\n        ])\n    else:\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a fact-checking expert. Review the draft answer against the key findings and source content.\n        Identify any claims that:\n        1. Are not supported by the sources\n        2. Contradict the sources\n        3. Need qualification or additional context\n        \n        Original Query: {query}\n        \n        Draft Answer:\n        {draft_answer}\n        \n        Key Findings from Research:\n        {key_findings}\n        \n        Citations Used:\n        {citations}\n        \n        Return your assessment in JSON format:\n        {{\n            \"accuracy_assessment\": \"overall assessment of factual accuracy\",\n            \"unsupported_claims\": [\"claim1\", \"claim2\", ...],\n            \"suggested_corrections\": [\"correction1\", \"correction2\", ...],\n            \"citation_errors\": [\"citation error 1\", ...],\n            \"verification_notes\": \"additional verification notes\"\n        }}\n        \"\"\")\n        \n        response = MODEL.invoke([\n            HumanMessage(content=prompt.format(\n                query=state.query,\n                draft_answer=state.draft_answer,\n                key_findings=key_findings,\n                citations=citations_context\n            ))\n        ])\n    \n    try:\n        # Extract JSON content\n        json_content = response.content\n        if \"```json\" in json_content:\n            json_content = json_content.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in json_content:\n            json_content = json_content.split(\"```\")[1].split(\"```\")[0].strip()\n            \n        state.verified_info = json.loads(json_content)\n    except:\n        state.verified_info = {\n            \"accuracy_assessment\": \"Unable to complete fact checking due to parsing error.\",\n            \"unsupported_claims\": [],\n            \"suggested_corrections\": [],\n            \"verification_notes\": \"Technical error occurred during verification.\"\n        }\n    \n    state.metadata[\"fact_checking_timestamp\"] = datetime.now().isoformat()\n    return state\n\ndef finalizing_agent(state: ResearchState) -> ResearchState:\n    \"\"\"Polish and finalize the answer based on fact checking and format appropriately.\"\"\"\n    # Prepare different contexts based on query type\n    if state.requires_metrics:\n        # For metrics queries, include the verification of numerical claims\n        number_verification = state.verified_info.get(\"number_verification\", [])\n        formatted_verification = \"\\n\".join([\n            f\"- Claim: \\\"{item.get('claim', '')}\\\"\\n  Status: {item.get('verification', '')}\\n  \"\n            f\"Correction: {item.get('correction', 'None')}\" \n            for item in number_verification\n        ]) if number_verification else \"No specific numerical claims verified.\"\n        \n        missing_metrics = \"\\n\".join([f\"- {metric}\" for metric in state.verified_info.get(\"missing_metrics\", [])])\n        \n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an expert editor specializing in data-driven content. Refine the draft answer \n        based on fact-checking feedback with special attention to numerical accuracy.\n        \n        Original Query: {query}\n        \n        Draft Answer:\n        {draft_answer}\n        \n        Fact-Checking Assessment:\n        Accuracy: {accuracy}\n        \n        Numerical Verification:\n        {number_verification}\n        \n        Missing Metrics:\n        {missing_metrics}\n        \n        Citation Errors:\n        {citation_errors}\n        \n        Suggested Corrections:\n        {corrections}\n        \n        Create a final polished answer that:\n        1. Addresses all fact-checking concerns, especially numerical accuracy\n        2. Presents quantitative data clearly, possibly using tables or structured lists\n        3. Includes proper in-text citations for all claims and metrics\n        4. Maintains clarity and readability\n        5. Directly answers the original query with emphasis on metrics and comparisons\n        6. Notes any important limitations in the available information\n        \n        Add a citation reference section at the end with all cited sources listed.\n        Format: [ID] Title - URL\n        \n        IMPORTANT: Ensure ALL numbers and metrics are correct and properly cited.\n        Use markdown formatting for readability (headers, lists, emphasis, tables).\n        \"\"\")\n        \n        citation_errors = \"\\n\".join([f\"- {error}\" for error in state.verified_info.get(\"citation_errors\", [])])\n        corrections = \"\\n\".join([f\"- {correction}\" for correction in state.verified_info.get(\"suggested_corrections\", [])])\n        \n        response = MODEL.invoke([\n            HumanMessage(content=prompt.format(\n                query=state.query,\n                draft_answer=state.draft_answer,\n                accuracy=state.verified_info.get(\"accuracy_assessment\", \"Unknown\"),\n                number_verification=formatted_verification,\n                missing_metrics=missing_metrics,\n                citation_errors=citation_errors,\n                corrections=corrections\n            ))\n        ])\n    else:\n        # Standard editing prompt for non-metric queries\n        unsupported_claims = \"\\n\".join([f\"- {claim}\" for claim in state.verified_info.get(\"unsupported_claims\", [])])\n        corrections = \"\\n\".join([f\"- {correction}\" for correction in state.verified_info.get(\"suggested_corrections\", [])])\n        citation_errors = \"\\n\".join([f\"- {error}\" for error in state.verified_info.get(\"citation_errors\", [])])\n        \n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an expert editor. Refine the draft answer based on fact-checking feedback.\n        \n        Original Query: {query}\n        \n        Draft Answer:\n        {draft_answer}\n        \n        Fact-Checking Assessment:\n        Accuracy: {accuracy}\n        Unsupported Claims: {unsupported_claims}\n        Suggested Corrections: {corrections}\n        Citation Errors: {citation_errors}\n        \n        Create a final polished answer that:\n        1. Addresses all fact-checking concerns\n        2. Maintains clarity and readability\n        3. Directly answers the original query\n        4. Notes any important limitations in the available information\n        5. Includes proper citations\n        \n        Add a citation reference section at the end with all cited sources listed.\n        Format: [ID] Title - URL\n        \n        Use markdown formatting for readability (headers, lists, emphasis).\n        \"\"\")\n        \n        response = MODEL.invoke([\n            HumanMessage(content=prompt.format(\n                query=state.query,\n                draft_answer=state.draft_answer,\n                accuracy=state.verified_info.get(\"accuracy_assessment\", \"Unknown\"),\n                unsupported_claims=unsupported_claims,\n                corrections=corrections,\n                citation_errors=citation_errors\n            ))\n        ])\n    \n    # Add reflection for comprehensiveness\n    answer = response.content\n    \n    reflection_prompt = ChatPromptTemplate.from_template(\"\"\"\n    Review this answer for completeness. Does it fully address the user's query?\n    \n    Original Query: {query}\n    \n    Final Answer:\n    {answer}\n    \n    Is there anything significant missing that would make this answer more complete?\n    If yes, add ONLY what's missing. If no, return the answer unchanged.\n    \n    Return only the final complete answer.\n    \"\"\")\n    \n    reflection_response = MODEL.invoke([\n        HumanMessage(content=reflection_prompt.format(\n            query=state.query,\n            answer=answer\n        ))\n    ])\n    \n    state.final_answer = reflection_response.content\n    state.metadata[\"completion_timestamp\"] = datetime.now().isoformat()\n    \n    return state\n\n# Conditional Edge Function\ndef needs_more_research(state: ResearchState) -> str:\n    \"\"\"Determine if additional research is needed based on information gaps and missing aspects.\"\"\"\n    # Check if critical information gaps were identified\n    gaps = state.analyzed_content.get(\"information_gaps\", [])\n    missing_aspects = state.analyzed_content.get(\"missing_aspects\", [])\n    \n    # Combine regular gaps with missing aspects\n    all_gaps = gaps + missing_aspects\n    \n    # Check for critical gaps (those that seem important)\n    critical_gap_indicators = [\"critical\", \"essential\", \"necessary\", \"important\", \"key\", \n                               \"significant\", \"crucial\", \"primary\", \"main\", \"major\"]\n    \n    critical_gaps = [gap for gap in all_gaps if any(term in gap.lower() for term in critical_gap_indicators)]\n    \n    # For metrics queries, check if we have enough quantitative data\n    if state.requires_metrics:\n        quant_data = state.analyzed_content.get(\"quantitative_data\", [])\n        has_sufficient_metrics = len(quant_data) >= 3  # Arbitrary threshold\n        \n        metric_gaps = [gap for gap in all_gaps if any(term in gap.lower() for term in \n                                                     [\"metric\", \"number\", \"statistic\", \"figure\",\n                                                      \"quantitative\", \"data\", \"benchmark\"])]\n        \n        if (not has_sufficient_metrics or metric_gaps) and len(state.search_queries) < 10:\n            # Generate metric-focused queries\n            prompt = ChatPromptTemplate.from_template(\"\"\"\n            Based on the current research, we need more specific metrics and quantitative data.\n            Generate 2-3 additional search queries focused on finding numerical data about:\n            \n            Original Query: {query}\n            \n            Missing Metrics/Data: {gaps}\n            \n            For each missing metric or data point, create a search query specifically designed \n            to find that numerical information. Format the queries to target reliable sources\n            of benchmarks and metrics.\n            \n            Return only the additional search queries as a JSON array:\n            [\"query1\", \"query2\", ...]\n            \"\"\")\n            \n            gaps_text = \"\\n\".join([f\"- {gap}\" for gap in all_gaps])\n            response = MODEL.invoke([\n                HumanMessage(content=prompt.format(\n                    query=state.query,\n                    gaps=gaps_text\n                ))\n            ])\n            \n            try:\n                # Extract JSON content\n                content = response.content\n                if \"```json\" in content:\n                    content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n                elif \"```\" in content:\n                    content = content.split(\"```\")[1].split(\"```\")[0].strip()\n    \n                new_queries = json.loads(content)\n                state.search_queries.extend(new_queries)\n                return \"needs_more_research\"\n            except:\n                return \"proceed_to_draft\"\n    \n    # Standard check for non-metric queries  \n    if critical_gaps and len(state.search_queries) < 8:\n        # Generate additional search queries based on gaps\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        Based on the current research, generate 2-3 additional search queries to fill these information gaps:\n        \n        Original Query: {query}\n        \n        Information Gaps: {gaps}\n        \n        Return only the additional search queries as a JSON array:\n        [\"query1\", \"query2\", ...]\n        \"\"\")\n        \n        gaps_text = \"\\n\".join([f\"- {gap}\" for gap in all_gaps])\n        response = MODEL.invoke([\n            HumanMessage(content=prompt.format(\n                query=state.query,\n                gaps=gaps_text\n            ))\n        ])\n        \n        try:\n            # Extract JSON content\n            content = response.content\n            if \"```json\" in content:\n                content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n            elif \"```\" in content:\n                content = content.split(\"```\")[1].split(\"```\")[0].strip()\n\n            new_queries = json.loads(content)\n            state.search_queries.extend(new_queries)\n            return \"needs_more_research\"\n        except:\n            return \"proceed_to_draft\"\n    else:\n        return \"proceed_to_draft\"\n\n# Graph Construction\ndef build_research_graph() -> StateGraph:\n    \"\"\"Build the research agent workflow graph with improved routing.\"\"\"\n    graph = StateGraph(ResearchState)\n    \n    # Add nodes\n    graph.add_node(\"planner\", planner_agent)\n    graph.add_node(\"search\", search_agent)\n    graph.add_node(\"content_extraction\", content_extraction_agent)\n    graph.add_node(\"analysis\", analysis_agent)\n    graph.add_node(\"drafting\", drafting_agent)\n    graph.add_node(\"fact_checking\", fact_checking_agent)\n    graph.add_node(\"finalizing\", finalizing_agent)\n    \n    # Connect nodes with directed edges\n    graph.add_edge(\"planner\", \"search\")\n    graph.add_edge(\"search\", \"content_extraction\")\n    graph.add_edge(\"content_extraction\", \"analysis\")\n    \n    # Add conditional edge from analysis\n    graph.add_conditional_edges(\n        \"analysis\",\n        needs_more_research,\n        {\n            \"needs_more_research\": \"search\",\n            \"proceed_to_draft\": \"drafting\"\n        }\n    )\n    \n    graph.add_edge(\"drafting\", \"fact_checking\")\n    graph.add_edge(\"fact_checking\", \"finalizing\")\n    graph.add_edge(\"finalizing\", END)\n    \n    # Set entry point\n    graph.set_entry_point(\"planner\")\n    \n    return graph.compile()\n\n# Main execution function\ndef run_research_agent(query: str) -> Dict[str, Any]:\n    \"\"\"Run the research agent pipeline on a given query.\"\"\"\n    print(f\"Starting research on: {query}\")\n    start_time = datetime.now()\n    \n    # Initialize state with query\n    state = ResearchState(query=query)\n    \n    # Build and run the graph\n    research_graph = build_research_graph()\n    memory_saver = MemorySaver()\n    result = research_graph.invoke(state)\n    \n    end_time = datetime.now()\n    execution_time = (end_time - start_time).total_seconds()\n    \n    # Format citations for return\n    citations = []\n    for citation in result[\"citations\"]:\n        citations.append({\n            \"id\": citation[\"id\"],\n            \"title\": citation[\"title\"],\n            \"url\": citation[\"url\"]\n        })\n    \n    # Return final result with enhanced metadata\n    return {\n        \"query\": query,\n        \"final_answer\": result[\"final_answer\"],\n        \"metadata\": {\n            \"execution_time\": {\n                \"started\": result[\"metadata\"].get(\"planning_timestamp\", \"\"),\n                \"completed\": result[\"metadata\"].get(\"completion_timestamp\", \"\"),\n                \"seconds_elapsed\": execution_time\n            },\n            \"search_strategy\": result[\"metadata\"].get(\"search_strategy\", \"\"),\n            \"search_queries\": result[\"search_queries\"],\n            \"requires_metrics\": result[\"metadata\"].get(\"requires_metrics\", False),\n            \"sources_count\": {\n                \"perplexity\": result[\"metadata\"].get(\"perplexity_sources_count\", 0),\n                \"tavily\": result[\"metadata\"].get(\"tavily_results_count\", 0),\n                \"total_extracted\": result[\"metadata\"].get(\"extracted_sources_count\", 0)\n            },\n            \"key_findings_count\": len(result[\"analyzed_content\"].get(\"key_findings\", [])),\n            \"citations\": citations\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    query = \"ok can you tell me the information about claude 3.7 sonnet and its new advanced features\"\n    result = run_research_agent(query)\n    \n    print(\"\\n\" + \"=\" * 50)\n    print(f\"QUERY: {query}\")\n    print(\"=\" * 50)\n    print(result[\"final_answer\"])\n    print(\"\\n\" + \"=\" * 50)\n    print(f\"Execution took {result['metadata']['execution_time']['seconds_elapsed']:.2f} seconds\")\n    print(f\"Sources used: {result['metadata']['sources_count']['total_extracted']} from Perplexity ({result['metadata']['sources_count']['perplexity']}) and Tavily ({result['metadata']['sources_count']['tavily']})\")\n    if result['metadata']['requires_metrics']:\n        print(\"Query was identified as requiring quantitative metrics.\")\n    print(\"=\" * 50)\n\n\n\n\n\n    ",
    "content_insights": [
      "Complex Workflow Management**: The system manages a complex workflow that includes planning, searching, content extraction, analysis, drafting, fact-checking, and finalizing, all coordinated through a state graph.",
      "AI Model Diversity**: The implementation utilizes multiple AI models (Claude, GPT-4o, Perplexity) for various tasks, showcasing the benefits of model diversity in achieving comprehensive research outcomes.",
      "Conditional Workflow**: The workflow includes conditional edges based on the outcome of the analysis step, allowing for dynamic adjustment of the research process."
    ],
    "ai_analysis": "",
    "current_blog": null,
    "current_critique": null,
    "blog_versions": [],
    "quality_scores": [],
    "user_requirements": "Professional and engaging",
    "feedback_history": [],
    "workflow_iterations": 0,
    "last_updated": "2025-10-25T17:45:23.605349"
  },
  "user_preferences": {},
  "created_at": "2025-10-25T17:44:23.069860",
  "last_updated": "2025-10-25T17:45:26.203464",
  "blogs_completed": 0
}